{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp/ybpCl7MdTiJgOxfJSTw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing a model using neural networks"
      ],
      "metadata": {
        "id": "v04qEMLw-XuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epG016j3-QSs"
      },
      "outputs": [],
      "source": [
        "# Packages used for developing models, as well as processing information\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from warnings import filterwarnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.pipeline import Pipeline,make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Reading the database and checking some information\n",
        "#I purposely eliminated some information from the data set so that I could apply methods to fill\n",
        "#in empty fields and transform text data into numeric values.\n",
        "\n",
        "Dataset = pd.read_csv('breast_cancer.csv',sep=';', on_bad_lines='skip')\n",
        "#Dataset.info()\n",
        "#Dataset.describe()"
      ],
      "metadata": {
        "id": "x9ZKLh-E-jkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Some Measurements"
      ],
      "metadata": {
        "id": "JL8_sw08_aRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.5,rc={'figure.figsize':(20,20)}) #usando a biblioteca sns posso verificar algumas distribuições dos meus dados\n",
        "eixo=Dataset.hist(bins=20,color='red')"
      ],
      "metadata": {
        "id": "ZZ_mPTmy-jmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Some Variables"
      ],
      "metadata": {
        "id": "WFub0Wuk_lTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select objetc columns and transform into float values\n",
        "def float_values(base,textos):\n",
        "   #textos =base.select_dtypes(include=['object']).columns\n",
        "   for column in textos:\n",
        "    if column not in ['target','Family Case']:\n",
        "      base[column] = base[column].str.replace(',', '.').astype(float)\n",
        "   base.select_dtypes(include=['object']).columns\n",
        "   return base\n",
        "\n",
        "#if my target is a text I transform in dummy value\n",
        "def target_variable(base):\n",
        "  base['target'] = np.where(base['target']=='Malignant', 1, 0)\n",
        "  return base\n",
        "\n",
        "# transform in dummies some variables\n",
        "def dummy_df(base, X,c):\n",
        "    Lista_variaveis = []\n",
        "    Lista_dummies = []\n",
        "\n",
        "    objetos = []\n",
        "    for column_name in base.columns:\n",
        "        if column_name in c:\n",
        "            objetos.append(column_name)\n",
        "\n",
        "    if objetos:  # Check if objetos is not empty\n",
        "        # Convert X to a DataFrame for easier manipulation\n",
        "        X_df = pd.DataFrame(X, columns=base.columns[:-1])\n",
        "\n",
        "        # Perform one-hot encoding using Pandas get_dummies\n",
        "        for col_name in objetos:\n",
        "            # Get the numerical index of the column\n",
        "            col_index = X_df.columns.get_loc(col_name)\n",
        "            dummy_df = pd.get_dummies(X_df.iloc[:, col_index], prefix=base.columns[col_index], dtype='int')\n",
        "\n",
        "            X_df = pd.concat([X_df, dummy_df], axis=1)\n",
        "            Lista_dummies.extend(dummy_df.columns)  # Add dummy column names\n",
        "\n",
        "        # Drop original categorical columns\n",
        "        X_df = X_df.drop(columns=objetos)  # Pass column names directly\n",
        "\n",
        "        # Update Lista_variaveis with dummy columns and remaining features\n",
        "        Lista_variaveis = list(X_df.columns[~X_df.columns.isin(Lista_dummies)]) + Lista_dummies\n",
        "        # Convert back to NumPy array if needed\n",
        "        X = X_df.values\n",
        "    return X, Lista_variaveis\n",
        "\n",
        "def normalize_df(X,Lista_variaveis):\n",
        "  #Normalize the data\n",
        "  X =pd.DataFrame(data=X, columns=Lista_variaveis)\n",
        "  min_max_scaler =MinMaxScaler()\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  return X\n",
        "\n"
      ],
      "metadata": {
        "id": "dUQ-n9Md-jpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textos =Dataset.select_dtypes(include=['object']).columns\n",
        "c=textos\n",
        "Dataset=float_values(Dataset,textos)\n",
        "Dataset=target_variable(Dataset)\n",
        "\n",
        "\n",
        "X = Dataset.iloc[:,:-1].values\n",
        "y = Dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "u_Pv5vBw6kCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In this step I select my data set and transform other text variables into numeric ones to facilitate the processing of my model.\n",
        "# Get the indices of columns with missing values\n",
        "missing_cols_indices = [Dataset.columns.get_loc(col) for col in Dataset.columns[Dataset.isna().any()]]\n",
        "\n",
        "# Impute missing values using these indices\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer = imputer.fit(X[:, missing_cols_indices])\n",
        "X[:, missing_cols_indices] = imputer.transform(X[:, missing_cols_indices])\n"
      ],
      "metadata": {
        "id": "XB4267bK-jvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Lista_variaveis=dummy_df(Dataset,X,c)\n",
        "X=normalize_df(X,Lista_variaveis)\n"
      ],
      "metadata": {
        "id": "k6SDHkJE6uvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development and Results"
      ],
      "metadata": {
        "id": "N2olQUyGCiBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural Network Model\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "# Convert ytrain and ytest to integer labels before fitting\n",
        "ytrain = ytrain.astype(int) # Convert to integer type to avoid \"unknown label type\" error.\n",
        "ytest = ytest.astype(int) # Convert to integer type to avoid \"unknown label type\" error.\n",
        "\n",
        "num_neu= len(Lista_variaveis)\n",
        "targ=2\n",
        "ocult_neu=int((num_neu*(2/3))+2)\n",
        "\n",
        "neuro=tf.keras.models.Sequential([tf.keras.layers.Dense(num_neu,input_shape=(len(Lista_variaveis),)\n",
        "                                               ,activation='relu',kernel_initializer='he_normal'),\n",
        "                           tf.keras.layers.Dropout(0.5),\n",
        "                           tf.keras.layers.Dense(ocult_neu,activation='relu',kernel_initializer='he_normal'),\n",
        "                           tf.keras.layers.Dropout(0.5),\n",
        "                           tf.keras.layers.Dense(2, activation='softmax')])"
      ],
      "metadata": {
        "id": "zpXOEQjq-jxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train setings\n",
        "neuro.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), #or binary_crossentropy\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_CtK66Jb-j0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 15"
      ],
      "metadata": {
        "id": "lfOALu8i-j25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuro.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA4XhQ0P-j5Z",
        "outputId": "a4a5a05f-f8c6-455f-9897-ee899026314c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5218 - loss: 0.9539\n",
            "Epoch 2/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 0.6992\n",
            "Epoch 3/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7059 - loss: 0.5618\n",
            "Epoch 4/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.5066\n",
            "Epoch 5/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8649 - loss: 0.3812\n",
            "Epoch 6/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8315 - loss: 0.3703\n",
            "Epoch 7/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8838 - loss: 0.3126\n",
            "Epoch 8/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2646\n",
            "Epoch 9/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8817 - loss: 0.2647\n",
            "Epoch 10/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9168 - loss: 0.2508\n",
            "Epoch 11/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9070 - loss: 0.2244\n",
            "Epoch 12/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9249 - loss: 0.2110\n",
            "Epoch 13/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.1407\n",
            "Epoch 14/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9129 - loss: 0.1903\n",
            "Epoch 15/15\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9052 - loss: 0.2022\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7be1382a1210>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neuro.evaluate(Xtest, ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gccrXrRI-j8R",
        "outputId": "1a16569f-d20b-4909-afdc-bc7add8b606c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9808 - loss: 0.0768  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09611456096172333, 0.9649122953414917]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.round(neuro.predict(Xtest, verbose=0)[:,1], 7)\n",
        "fpr, tpr, thresholds = roc_curve(ytest, probs)\n",
        "#Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n",
        "#This test compares the underlying continuous distributions F(x) and G(x) of two independent samples\n",
        "print('KS  Validation: {0:.2f}% e AUC: {1:.2f}%'.format(stats.ks_2samp(fpr, tpr)[0] * 100.0, auc(fpr, tpr) * 100))\n",
        "print((probs.max(),probs.min()))\n",
        "\n",
        "probs = np.round(neuro.predict(Xtrain, verbose=0)[:,1], 7)\n",
        "fpr, tpr, thresholds = roc_curve(ytrain, probs)\n",
        "print('KS Development: {0:.2f}% e AUC: {1:.2f}%'.format(stats.ks_2samp(fpr, tpr)[0] * 100.0, auc(fpr, tpr) * 100))\n",
        "print((probs.max(),probs.min()))\n",
        "\n",
        "\n",
        "\n",
        "# Define a scoring function for permutation_importance\n",
        "def scoring_fn(estimator, X, y):\n",
        "    y_pred = np.argmax(estimator.predict(X, verbose=0), axis=1)  # Get predicted classes\n",
        "    return accuracy_score(y, y_pred)  # Calculate accuracy\n",
        "\n",
        "# Calculate permutation feature importance using the scoring function\n",
        "result = permutation_importance(\n",
        "    neuro, Xtest, ytest, n_repeats=10, random_state=0, scoring=scoring_fn\n",
        ")\n",
        "# Create a DataFrame to store the results\n",
        "feature_importances = pd.DataFrame(\n",
        "    {\n",
        "        \"feature\": Lista_variaveis,  # Assuming Lista_variaveis contains feature names\n",
        "        \"importance\": result.importances_mean,\n",
        "    }\n",
        ").sort_values(\"importance\", ascending=False)\n",
        "\n",
        "\n",
        "print(feature_importances) # Print the top 5 most important features or change the number to see others"
      ],
      "metadata": {
        "id": "Mje4WbTYH275"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=neuro.predict(X, verbose=0)\n",
        "y_pred = [np.argmax(v) for v in y_pred]\n",
        "X1 = min_max_scaler.inverse_transform(X)\n",
        "df = pd.DataFrame(data=X1, columns=Lista_variaveis)\n",
        "df2 = pd.DataFrame(data=y,columns=['Tarq'])\n",
        "df3 = pd.DataFrame(data=y_pred,columns=['Pred'])\n",
        "df = pd.concat([df,df2,df3],axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "_Z2CJ0RXJ-Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34i-TYT34G26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Pipelines\n"
      ],
      "metadata": {
        "id": "JZ6QyoLe4H3q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dM6oYEFZ4HQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}